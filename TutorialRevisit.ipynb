{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial Revisit\n",
    "\n",
    "In this Notebook the Tutorials will be revisited and described in a few paragraphs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tutorial 1\n",
    "\n",
    "In the first tutorial, we get a glimpse of what a neural network actually is, and what some fundamental principles mean - namely backpropagation, an activation function, the loss and how to implement a simple neural network with Tensorflow. All that is shown with the 'hello world' of deep learning - the MNIST dataset.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "Backpropagation is a fundamental part of fitting a neural network, to get the network to learn properly, we have to look at our expected result and the result the network has given. We then compare these and go from last to first layer to change the weights and biases at each node, so our Loss gets minimized.\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "The activation function is a mathematical function that is applied to the output of a neuron in a neural network. The purpose of the activation function is to introduce nonlinearity into the network, allowing the model to learn more complex relationships between the input features and the output. There are four widely used ones: Sigmoid, Tanh, ReLU and Leaky ReLU. Each activation function has its own properties and is suitable for different types of tasks.\n",
    "### Loss function\n",
    "\n",
    "To determine our loss, we need a loss function: It is a scalar value which we get from comparing the predicted outcome with the desired output. The loss function we use in the first example is the least squares approximation.\n",
    "### Implementation with Tensorflow\n",
    "\n",
    "With Tensorflow, a library provided by Google to make development of neural networks easier and focus on deeper topics, such as diving into different architectures and adding more layers.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tutorial 2\n",
    "The second Tutorial is all about reusing a pretrained model (transfer learning) for the classification of pictures of cats or dogs.\n",
    "Transfer learning is a technique in which a pre-trained model, which has been trained on a large and general dataset, is used as the basis for a model specific to a new task. Here it is used for feature extraction.\n",
    "### Feature extraction\n",
    "Feature extraction involves using the representations learned by the pre-trained model to extract meaningful features from new samples, and adding a new output layer on top of the pre-trained model to be trained from scratch for the new task and output space. In this tutorial, the Dogs vs Cats dataset from Kaggle is used to demonstrate transfer learning using a pre-trained model for object classification.\n",
    "### Code overview\n",
    "The necessary libraries are imported and the dataset is prepared for training and validation. The pre-trained model is then used for feature extraction and the extracted features are fed into a newly added classifier. The classifier is trained and the model is tested on the validation set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial 3\n",
    "This tutorial focuses on the use-case of a U-Net-Architecture for cell boundary segmentation. Motivation for this is:\n",
    "Segmentation networks and convolutional neural networks (CNNs) typically require a large amount of training data to perform well, but it is important to use this data efficiently. Segmentation involves identifying and classifying objects within an image, which requires a strong understanding of the meaning of the image. The U-Net model is effective at using a limited amount of training data to both classify objects and accurately locate them within an image.\n",
    "\n",
    "### Architecture\n",
    "A U-Net is a neural network model that has two separate paths for processing an input image: a contracting path and an expanding path. The contracting path consists of several convolutional layers that shrink the spatial resolution of the image, while the expanding path is made up of transposed convolutions that increase the resolution. These two paths are connected by skip connections, which help the network retain spatial information and establish a connection between the low-level features of the input image and the high-level features of the output mask.\n",
    "\n",
    "### Implementation\n",
    "To implement the U-Net we have to define the model and not use the standard seuqential model builder from Keras, because otherwise the shape wouldn't be possible. Later the model gets trained and tested by the respective Generator methods."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "*so long and thanks for all the fish*"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
